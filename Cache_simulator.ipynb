{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Google Drive initializaton "
      ],
      "metadata": {
        "id": "BJI6qXeyozYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "colab = True #If you are running colab\n",
        "if colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount= True)\n",
        "    try:\n",
        "        path = '/content/drive/MyDrive/Operating System Project'\n",
        "        os.chdir(path)\n",
        "    except:\n",
        "        path = '/content/drive/MyDrive/FIU/Classes/Operative Systems/Final Project' # insert your path to the shared folder\n",
        "    os.chdir(path)\n",
        "!ls"
      ],
      "metadata": {
        "id": "kfJNT76BEdpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library Imports"
      ],
      "metadata": {
        "id": "IetlRdEfo7o6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import random, string\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.rcParams.update({'font.size': 18, 'font.family': 'STIXGeneral', 'mathtext.fontset':'cm'})"
      ],
      "metadata": {
        "id": "OO5ljKfWo7XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split,LearningCurveDisplay, learning_curve\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "save = False # if you want to save figures"
      ],
      "metadata": {
        "id": "eS2xsx0HxNT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Cache` object\n",
        "Creates a simple cache object for simulation purposes. Considers a cache with slots available for caching files. One file per slot, we don't consider allocation properties.\n",
        "\n",
        "It requires three parameters:\n",
        "\n",
        "-`memory_size`: Maximum amount of elements on the cache\n",
        "\n",
        "-`set_function`: A function in charge of performing the cache replacement. It has to recieve:\n",
        "- `Cache`: A `Cache` object\n",
        "- `timestamp`: Time when the key was requested\n",
        "- `key`:       Name of element requested by the cache\n",
        "- `value` (optional): Additional value. For instnace, probability of being \n",
        "requested next time\n",
        "\n",
        "-`key list`: List with names of the cached elements\n",
        "\n",
        "We implemented LRU (Least Recently Used) MRU (Most Recently Used) cache policies.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sY43dgB_audM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Cache:\n",
        "    def __init__(self, memory_size, set_function, key_list):\n",
        "        \n",
        "        self.size = memory_size\n",
        "        self.cachevalue = {} # Value associated to the file (for example, probability)\n",
        "        self.timestamps = {} # Last time updated/requested\n",
        "        self.set_function = set_function\n",
        "        self.timelastuse = {key: 0 for key in key_list } # Last time updated/requested for each\n",
        "        self.usenumber = {key:0 for key in key_list } #stores how many times the key has been requested\n",
        "\n",
        "        self.cache_hit = 0\n",
        "        self.cache_miss = 0\n",
        "        self.cache_hit_performance = np.empty(shape=(0, ))\n",
        "        self.time = 0\n",
        "        self.views = {key: 0 for key in key_list} # Number of views for each movie\n",
        "\n",
        "    def Get(self, timestamp, key, value = None ): # Queries an element from the cache memory, returns the file if it is in cache, else it returns -1\n",
        "        self.usenumber[key] += 1\n",
        "        self.timelastuse[key] = timestamp-self.timelastuse[key]\n",
        "        if key in self.cachevalue:\n",
        "            self.timestamps[key] = timestamp\n",
        "            if value is not None:\n",
        "              self.cachevalue[key]['value'] = value\n",
        "            return self.cachevalue[key]\n",
        "        return -1\n",
        "    \n",
        "    def Set(self, timestamp, key, value=None):\n",
        "        return self.set_function(self, timestamp, key, value)\n",
        "\n",
        "    def Simulate(self, inqury, value = None, view_increment = 1):\n",
        "        content = self.Get(self.time, inqury, value)\n",
        "        if content == -1:\n",
        "\n",
        "            self.cache_miss += 1\n",
        "            self.Set(self.time, inqury, value)\n",
        "            \n",
        "        else:\n",
        "            self.cache_hit += 1\n",
        "            \n",
        "        \n",
        "\n",
        "        # Sort cache based on 'views' attribute\n",
        "        self.cache_hit_performance = np.hstack((self.cache_hit_performance, self.cache_hit))\n",
        "        self.time += 1\n",
        " \n",
        "\n",
        "    \n",
        "\"\"\"With cache hit, view will be increased and the the cache will be sorted based on it. \n",
        "It means most popular content will be at the top\"\"\"\n",
        "   \n",
        "\n",
        "'''Replacement functions. Each function has to recieve:\n",
        "- A Cache object\n",
        "- A timestamp: time when the key was requested\n",
        "- A key:       Name of element requested by the cache\n",
        "- A value (optional): Additional value. For instnace, probability of being requested next time\n",
        "\n",
        "It makes the replacement, replaces and old key with the new key\n",
        "'''\n",
        "\n",
        "def Set_LRU(cache_obj,timestamp, key, value=None): # Replacement policy if a change has to be made, in this case it is LRU. However, it can be changed\n",
        "    if len(cache_obj.cachevalue) >= cache_obj.size:\n",
        "\n",
        "        # find the oldest entry in the cache\n",
        "        oldest_key = min(cache_obj.timestamps, key=cache_obj.timestamps.get)\n",
        "\n",
        "        # take out the oldest key\n",
        "        cache_obj.cachevalue.pop(oldest_key)\n",
        "        cache_obj.timestamps.pop(oldest_key)\n",
        "\n",
        "    # update values, regardless if it was a cache hit or not\n",
        "    cache_obj.cachevalue[key] = value\n",
        "    cache_obj.timestamps[key] = timestamp\n",
        "\n",
        "\n",
        "def Set_MRU(cache_obj,timestamp, key, value=None): # Replacement policy if a change has to be made, in this case it is MRU. However, it can be changed\n",
        "    if len(cache_obj.cachevalue) >= cache_obj.size:\n",
        "        \n",
        "        # find the oldest entry in the cache\n",
        "        newest_key = max(cache_obj.timestamps, key=cache_obj.timestamps.get)\n",
        "\n",
        "        # take out the oldest key\n",
        "        cache_obj.cachevalue.pop(newest_key)\n",
        "        cache_obj.timestamps.pop(newest_key)\n",
        "\n",
        "    # update values, regardless if it was a cache hit or not\n",
        "    cache_obj.cachevalue[key] = {'value': value, 'views': 0}\n",
        "    cache_obj.timestamps[key] = timestamp"
      ],
      "metadata": {
        "id": "-xRAteU3a48S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Simulation example\n",
        "\n",
        "Creates 100 random words and simulate a Cache memory with `memory_size` size and simulates `n_inquiries` requests drawn at uniformly random."
      ],
      "metadata": {
        "id": "x_dVjXuwU2Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_list = []\n",
        "for _ in range(100):\n",
        "    words_list.append(''.join(random.choices(string.ascii_uppercase + string.digits, k=10)))"
      ],
      "metadata": {
        "id": "xNt-kale_WoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simulation\n",
        "n_inquiries, memory_size = 10000, 10\n",
        "\n",
        "# cache_memory, time, cache_hit, cache_miss, cache_hit_performance = Initialize(n_inquiries, memory_size,words_list, Set_MRU)\n",
        "cache_memory = Cache(memory_size= memory_size, set_function =Set_LRU, key_list =words_list)\n",
        "\n",
        "cache_inquries = random.choices(words_list, k = n_inquiries)\n",
        "\n",
        "for inqury in cache_inquries:\n",
        "    cache_memory.Simulate(inqury)\n",
        "\n",
        "\n",
        "cache_hit, cache_miss, cache_hit_performance, time = cache_memory.cache_hit, cache_memory.cache_miss, cache_memory.cache_hit_performance, cache_memory.time\n",
        "\n",
        "print(f'Cache hits {cache_hit}\\n Cache misses {cache_miss}')\n",
        "\n",
        "#plots\n",
        "f, (ax1,ax2) = plt.subplots(1, 2, figsize=(15,4))\n",
        "ax1.plot(100*cache_hit_performance/n_inquiries)\n",
        "ax1.set_title('Cache saturation through execution')\n",
        "ax1.set_xlabel('Inquiry')\n",
        "ax1.set_ylabel('Cache hit [%]')\n",
        "ax2.set_title('Relative cache')\n",
        "ax2.plot(100*cache_hit_performance/ range(1,n_inquiries+1))\n",
        "ax2.set_xlabel('Inquiry')\n",
        "ax2.set_ylabel('Current cache hit [%]')\n",
        "\n",
        "# print(len(cache_memory.usenumber), len(cache_memory.timelastuse))\n",
        "\"\"\" Note that this simulation doesn't involve the movie dataset or the popularity feature, as it is a separate demonstration of the cache functionality.\"\"\""
      ],
      "metadata": {
        "id": "WzobGgwn9HDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading dataset\n",
        "Reads an preprocessed IMdB dataset `Toy dataset.csv`, which is sorted by the most reviewed movies and shows the `n_movies` most reviewed ones. "
      ],
      "metadata": {
        "id": "MqB_OoYG1zEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df = pd.read_csv('Toy dataset.csv')\n",
        "dataset_df = dataset_df.drop('endYear', axis =1)\n",
        "dataset_df = dataset_df.replace('\\\\N',0)\n",
        "n_movies = 500 # the most reviewed movies\n",
        "dataset_df= dataset_df[:n_movies]\n",
        "fig = plt.figure(figsize=(8,5))\n",
        "plt.plot(dataset_df['numVotes'])\n",
        "plt.title('Movie distribution by popularity')\n",
        "plt.xlabel(r'$n^{th}$ most popular movie')\n",
        "plt.ylabel('Number of votes')\n",
        "dataset_df.head(10)\n",
        "if save:\n",
        "    fig.savefig('movie distribution.eps', format ='eps',bbox_inches='tight', transparent = True)"
      ],
      "metadata": {
        "id": "LlzvGcvP6T4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature extractors\n",
        "\n",
        "Auxiliar functions that obtain features to be used in the ML models. They inclde columns such that `'averageRating','runtimeMinutes', 'isAdult', 'startYear'`"
      ],
      "metadata": {
        "id": "f004doKvRDan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def One_hot(values, length= n_movies):\n",
        "\n",
        "    encoding = [0.0]*length\n",
        "    for value in values:\n",
        "        encoding[value]= 1.0\n",
        "    return encoding\n",
        "\n",
        "   \n",
        "def get_popularity(cache_obj, movie, days=30):\n",
        "    current_time = cache_obj.time\n",
        "    start_time = max(current_time - days, 0)\n",
        "    views = [cache_obj.views[movie] for t in range(start_time, current_time)]\n",
        "    popularity = sum(views) / days\n",
        "    return popularity\n",
        "\n",
        "\n",
        "\n",
        "def Obtain_features(cache_obj, df, current_inquiry, \n",
        "                    cache_inquiries):\n",
        "    # current inquiry is an index\n",
        "    time = cache_obj.time\n",
        "    num_negative_per_positive = 1\n",
        "    last_inquiries = cache_inquiries[time-num_of_past_inquiries:time]\n",
        "\n",
        "    feature_list = [Obtain_individual_features(cache_obj, df, current_inquiry, \n",
        "                    last_inquiries)]\n",
        "\n",
        "    label_list = [1.0]\n",
        "    index_list = np.array(list(converter_dict.keys()))\n",
        "    del_list = np.delete(index_list, current_inquiry)\n",
        "    neg_class = np.random.choice(del_list,size=num_negative_per_positive, replace=False)\n",
        "\n",
        "    for non_movie in neg_class:\n",
        "\n",
        "        \n",
        "        feature_list.append(Obtain_individual_features(cache_obj, df, non_movie, \n",
        "                    last_inquiries))\n",
        "        label_list += [-1.0]\n",
        "\n",
        "    return feature_list, label_list\n",
        "\n",
        "def Obtain_individual_features(cache_obj, df, current_inquiry, \n",
        "                    last_inquiries):\n",
        "    columns = ['averageRating','runtimeMinutes', 'isAdult', 'startYear' ]\n",
        "    num_of_past_inquiries = 5\n",
        "    popularity = get_popularity(cache_obj, current_inquiry)\n",
        "    lst = One_hot([current_inquiry]) + [cache_obj.timelastuse[current_inquiry],\n",
        "            cache_obj.usenumber[current_inquiry], popularity] +  df.loc[current_inquiry,columns].to_list()\n",
        "\n",
        "    lst +=  One_hot(last_inquiries)\n",
        "    return lst\n"
      ],
      "metadata": {
        "id": "3TOytOh4RCxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulating cache accesses\n",
        "Simulates cache accesses which drawing samples at random from the distribution defined by their probability in order to have a dataset to train a ML model."
      ],
      "metadata": {
        "id": "UOWSMb-l7AI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_inquiries, memory_size, num_of_past_inquiries = 20000, 25, 5\n",
        "\n",
        "cache_memory = Cache(memory_size,Set_LRU, range(n_movies))\n",
        "\n",
        "probability_distribution = np.array(dataset_df['numVotes']/(dataset_df['numVotes'].sum()))\n",
        "\n",
        "np.random.seed(12345)\n",
        "cache_inquiries = np.random.choice(range(n_movies), n_inquiries, p = probability_distribution )\n",
        "  \n",
        "\n",
        "converter_dict = {index : movie for movie, index in zip(dataset_df['originalTitle'].to_list(),range(n_movies) )}\n",
        "\n",
        "\n",
        "X_dataset, y_dataset = [], []\n",
        "\n",
        "for movie in cache_inquiries:\n",
        "    cache_memory.Simulate(movie)\n",
        "    if cache_memory.time>num_of_past_inquiries-1:\n",
        "\n",
        "        # print(last_inquries)\n",
        "        X, y = Obtain_features(cache_memory,dataset_df, movie,\n",
        "                            cache_inquiries)\n",
        "        X_dataset += X\n",
        "        y_dataset += y\n",
        "    if cache_memory.time %100==0:\n",
        "        clear_output(wait=True)\n",
        "        print(f'Query number {cache_memory.time}')\n",
        "\n",
        "X_dataset, y_dataset = np.array(X_dataset, dtype=np.float32), np.array(y_dataset, dtype=np.float32)\n",
        "\n",
        "print(f'Dataset with {X_dataset.shape[0]} samples and {X_dataset.shape[1]} features each.')"
      ],
      "metadata": {
        "id": "EpXYv0tM63I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a ML model"
      ],
      "metadata": {
        "id": "LJBA6P2DcoaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "proportion = 0.8\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset,train_size = proportion)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "JAQudiCJkJ_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot results simulated cache access"
      ],
      "metadata": {
        "id": "OH_V6hZeaow3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache_hit, cache_miss, cache_hit_performance, time = cache_memory.cache_hit, cache_memory.cache_miss, cache_memory.cache_hit_performance, cache_memory.time\n",
        "print(f'Cache hits: {cache_hit} \\nCache misses: {cache_miss}')\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(100*cache_hit_performance/n_inquiries)\n",
        "plt.title('Cache hit saturation')\n",
        "plt.ylabel('Relative hit [%]')\n",
        "plt.xlabel('Query number')\n",
        "fig = plt.figure(figsize=(7,4))\n",
        "plt.plot(100*cache_hit_performance/ range(1,n_inquiries+1))\n",
        "plt.title('Cache hit classic LRU')\n",
        "plt.ylabel('Cache hit rate [%]')\n",
        "plt.xlabel('Query number')\n",
        "print(X_dataset.shape, y_dataset.shape)\n",
        "print(f'Memory size {memory_size}')\n",
        "if save:\n",
        "    fig.savefig(f'OS_Only_LRU_Cache_Hit_with_{n_inquiries}.eps', format ='eps',bbox_inches='tight', transparent = True)"
      ],
      "metadata": {
        "id": "VJkNcoQpanz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning models"
      ],
      "metadata": {
        "id": "_zACP9IVw8N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def get_model(kind='knn'):\n",
        "    if kind == 'svm':\n",
        "        model = make_pipeline(StandardScaler(), SVC(C=10.0**(2), gamma='auto',verbose =True, probability =True))\n",
        "    elif kind == 'mlp':\n",
        "        model = make_pipeline(StandardScaler(), MLPClassifier(hidden_layer_sizes=(128,64,),verbose =False))\n",
        "    elif kind == 'forest':\n",
        "        model = make_pipeline(StandardScaler(), RandomForestClassifier(max_features= 'auto', max_depth = 70))\n",
        "    elif kind == 'knn':\n",
        "        model = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=10)) #number 10 is giving better accuracy in test and train set\n",
        "    \n",
        "    return model\n",
        "kind = 'mlp'\n",
        "model = get_model(kind)\n",
        "proportion = 0.8\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset,train_size = proportion)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape) "
      ],
      "metadata": {
        "id": "S94QmPMXklxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "hrVE5qX8yxmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = model.predict(X_test)\n",
        "test_acc, train_acc = accuracy_score(y_predicted, y_test), accuracy_score(y_train, model.predict(X_train))\n",
        "\n",
        "print(f'training acc {train_acc}. test acc {test_acc}')"
      ],
      "metadata": {
        "id": "I1RbyLeBz5x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning rates"
      ],
      "metadata": {
        "id": "1fDhwJnDbbY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_curv = True\n",
        "if learning_curv:\n",
        "    train_sizes, train_scores, test_scores = learning_curve(model, X_train, y_train)\n",
        "    display = LearningCurveDisplay(train_sizes=train_sizes,\n",
        "                                train_scores=train_scores, test_scores=test_scores)\n",
        "    fig,ax =plt.subplots(figsize=(8,5))\n",
        "    display.plot(ax, score_type ='both')\n",
        "    ax.set_title(f'Learning Curves {kind.upper()} model')\n",
        "    ax.set_xlabel('Number of samples/ Number of cache queries')\n",
        "    ax.set_ylabel('Accuracy')\n",
        "    if save:\n",
        "        fig.savefig(f'{kind.upper()}_Learning_Curve_Accuracy_in_{n_inquiries}.svg', format ='svg',bbox_inches='tight', transparent = True)"
      ],
      "metadata": {
        "id": "55k9HKvav39l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New cache replacement policies"
      ],
      "metadata": {
        "id": "xXkS99oMhZnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Set_ML_MRU(model, df = dataset_df,cache_inquiries = cache_inquiries, num_of_past_inquiries =num_of_past_inquiries): \n",
        "\n",
        "    def set_function(cache_obj,timestamp, key, value =None):\n",
        "        last_inquiries =  cache_inquiries[timestamp-num_of_past_inquiries:timestamp]\n",
        "        if len(cache_obj.cachevalue) >= cache_obj.size:\n",
        "            for movie in cache_obj.cachevalue.keys():\n",
        "                features = Obtain_individual_features(cache_obj, df, movie, \n",
        "                    last_inquiries)\n",
        "                features = np.array([features])\n",
        "                cache_obj.cachevalue[movie] = model.predict_proba(features)[0,0]\n",
        "\n",
        "            most_likely_key = max(cache_obj.cachevalue, key=cache_obj.cachevalue.get)\n",
        "\n",
        "\n",
        "            # take out the oldest key\n",
        "            cache_obj.cachevalue.pop(most_likely_key)\n",
        "            cache_obj.timestamps.pop(most_likely_key)\n",
        "\n",
        "        # update values, regardless if it was a cache hit or not\n",
        "        cache_obj.cachevalue[key] = {'value': value, 'views': 0}\n",
        "        cache_obj.timestamps[key] = timestamp\n",
        "    return set_function\n",
        "\n",
        "def Set_ML_LRU(model, df = dataset_df,cache_inquiries = cache_inquiries, num_of_past_inquiries =num_of_past_inquiries): \n",
        "\n",
        "    def set_function(cache_obj,timestamp, key, value =None):\n",
        "        last_inquiries = last_inquiries = cache_inquiries[timestamp-num_of_past_inquiries:timestamp]\n",
        "        if len(cache_obj.cachevalue) >= cache_obj.size:\n",
        "            for movie in list(cache_obj.cachevalue.keys()):                \n",
        "                features = Obtain_individual_features(cache_obj, df, movie, \n",
        "                    last_inquiries )\n",
        "                features = np.array([features])\n",
        "                cache_obj.cachevalue[movie] = model.predict_proba(features)[0,1]\n",
        "                # print(model.predict_proba(features))\n",
        "\n",
        "            \n",
        "            least_likely_key = min(cache_obj.cachevalue, key=cache_obj.cachevalue.get)\n",
        "            # print(f'k {most_likely_key}')\n",
        "\n",
        "            # take out the oldest key\n",
        "            cache_obj.cachevalue.pop(least_likely_key)\n",
        "            cache_obj.timestamps.pop(least_likely_key)\n",
        "\n",
        "        # update values, regardless if it was a cache hit or not\n",
        "        cache_obj.cachevalue[key] = {'value': value, 'views': 0}\n",
        "        cache_obj.timestamps[key] = timestamp\n",
        "    return set_function"
      ],
      "metadata": {
        "id": "WA_02usQhcwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cache replacement aided by ML simulation"
      ],
      "metadata": {
        "id": "SeztOTvdbp0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache_memory = Cache(memory_size, Set_ML_LRU(model), range(n_movies))\n",
        "\n",
        "\n",
        "for movie in cache_inquiries:\n",
        "    cache_memory.Simulate(movie)\n",
        "    if cache_memory.time%100==0:\n",
        "        clear_output(wait=True)\n",
        "        print(f'Query number {cache_memory.time}')\n",
        "\n",
        "\n",
        "cache_hit, cache_miss, cache_hit_performance, time = cache_memory.cache_hit, cache_memory.cache_miss, cache_memory.cache_hit_performance, cache_memory.time\n"
      ],
      "metadata": {
        "id": "MjBsBxgcWB64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot results of aided cache by ML"
      ],
      "metadata": {
        "id": "mWk7viTLbMbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache_hit, cache_miss, cache_hit_performance, time = cache_memory.cache_hit, cache_memory.cache_miss, cache_memory.cache_hit_performance, cache_memory.time\n",
        "print(f'Cache hits: {cache_hit} \\nCache misses: {cache_miss}')\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(100*cache_hit_performance/n_inquiries)\n",
        "plt.title('Cache hit saturation')\n",
        "plt.ylabel('Relative hit [%]')\n",
        "plt.xlabel('Query number')\n",
        "fig = plt.figure(figsize=(7,4))\n",
        "plt.plot(100*cache_hit_performance/ range(1,n_inquiries+1))\n",
        "plt.title(f'Cache hit LRU aided by {kind.upper()}')\n",
        "plt.ylabel('Cache hit rate [%]')\n",
        "plt.xlabel('Query number')\n",
        "if save:\n",
        "    fig.savefig(f'{kind.upper()}_LRU_Cache_Hit_{n_inquiries}.eps', format ='eps',bbox_inches='tight', transparent = True)\n",
        "print(X_dataset.shape, y_dataset.shape)\n",
        "print(f'Memory size {memory_size}')"
      ],
      "metadata": {
        "id": "Bsuvn5IvbLS8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}